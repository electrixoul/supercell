"""
Simple 3-Layer MLP for CIFAR-10 Classification
ä½¿ç”¨æ ‡å‡†BPç®—æ³•åœ¨CIFAR-10æ•°æ®é›†ä¸Šè®­ç»ƒç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº

Architecture:
- Input: 32x32x3 = 3072 features (flattened)  
- Hidden: 128 neurons with ReLU activation
- Output: 10 classes (softmax)

Hardware: MacBook Pro M4 Pro with MPS acceleration
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def set_device():
    """è®¾ç½®M4 Proçš„æœ€ä¼˜è®¾å¤‡"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… ä½¿ç”¨MPS (Metal Performance Shaders) åŠ é€Ÿ - M4 Pro")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… ä½¿ç”¨CUDAåŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("âš ï¸  ä½¿ç”¨CPU")
    return device

class SimpleMLP(nn.Module):
    """ç®€å•çš„ä¸‰å±‚MLP"""
    def __init__(self, input_size=3072, hidden_size=128, num_classes=10):
        super(SimpleMLP, self).__init__()
        
        # ç¬¬ä¸€å±‚ï¼šè¾“å…¥å±‚åˆ°éšè—å±‚
        self.fc1 = nn.Linear(input_size, hidden_size)
        
        # ç¬¬äºŒå±‚ï¼šéšè—å±‚åˆ°è¾“å‡ºå±‚
        self.fc2 = nn.Linear(hidden_size, num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æ ‡å‡†æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        # å±•å¹³è¾“å…¥ (batch_size, 3, 32, 32) -> (batch_size, 3072)
        x = x.view(x.size(0), -1)
        
        # ç¬¬ä¸€å±‚ + ReLUæ¿€æ´»
        x = F.relu(self.fc1(x))
        
        # ç¬¬äºŒå±‚ï¼ˆè¾“å‡ºå±‚ï¼‰
        x = self.fc2(x)
        
        return x

def train_epoch(model, device, train_loader, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        
        # åå‘ä¼ æ’­ (BPç®—æ³•)
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)
        
        # æ‰“å°è¿›åº¦
        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data):5d}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):3.0f}%)] '
                  f'Loss: {loss.item():.6f}')
    
    avg_loss = train_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy

def test_epoch(model, device, test_loader):
    """æµ‹è¯•ä¸€ä¸ªepoch"""
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    
    test_loss /= total
    accuracy = 100. * correct / total
    
    return test_loss, accuracy

def plot_learning_curves(train_losses, train_accs, test_losses, test_accs, save_path='learning_curves.png'):
    """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
    ax1.plot(epochs, test_losses, 'r-', label='æµ‹è¯•æŸå¤±', linewidth=2)
    ax1.set_title('æŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, train_accs, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    ax2.plot(epochs, test_accs, 'r-', label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2)
    ax2.set_title('å‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š å­¦ä¹ æ›²çº¿å·²ä¿å­˜è‡³: {save_path}")

def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print("="*50)
    print("æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print("="*50)
    
    return total_params, trainable_params

def main():
    print("ğŸš€ å¼€å§‹è®­ç»ƒç®€å•MLPåœ¨CIFAR-10æ•°æ®é›†ä¸Š")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = set_device()
    
    # è¶…å‚æ•°
    batch_size = 128
    test_batch_size = 256
    epochs = 20
    learning_rate = 0.001
    
    print(f"ğŸ“‹ è¶…å‚æ•°è®¾ç½®:")
    print(f"  æ‰¹é‡å¤§å°: {batch_size}")
    print(f"  æµ‹è¯•æ‰¹é‡: {test_batch_size}")
    print(f"  è®­ç»ƒè½®æ¬¡: {epochs}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print("="*60)
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # å½’ä¸€åŒ–åˆ°[-1, 1]
    ])
    
    # ä¸‹è½½å¹¶åŠ è½½CIFAR-10æ•°æ®é›†
    print("ğŸ“¥ ä¸‹è½½CIFAR-10æ•°æ®é›†...")
    train_dataset = datasets.CIFAR10(root='./data', train=True,
                                   download=True, transform=transform)
    test_dataset = datasets.CIFAR10(root='./data', train=False,
                                  download=True, transform=transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=True, num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=test_batch_size,
                           shuffle=False, num_workers=2, pin_memory=True)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
    print(f"  æµ‹è¯•æ ·æœ¬: {len(test_dataset):,}")
    print(f"  ç±»åˆ«æ•°é‡: 10")
    print("="*60)
    
    # CIFAR-10ç±»åˆ«åç§°
    classes = ('é£æœº', 'æ±½è½¦', 'é¸Ÿ', 'çŒ«', 'é¹¿', 'ç‹—', 'é’è›™', 'é©¬', 'èˆ¹', 'å¡è½¦')
    print(f"ğŸ·ï¸  ç±»åˆ«: {', '.join(classes)}")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleMLP(input_size=32*32*3, hidden_size=128, num_classes=10).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    count_parameters(model)
    
    # ä¼˜åŒ–å™¨ (æ ‡å‡†SGDæˆ–Adaméƒ½å¯ä»¥ï¼Œè¿™é‡Œç”¨Adam)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒè®°å½•
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []
    
    best_test_acc = 0.
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    total_start_time = time.time()
    
    for epoch in range(1, epochs + 1):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, epoch)
        
        # æµ‹è¯•
        test_loss, test_acc = test_epoch(model, device, test_loader)
        
        # è®°å½•
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), 'best_mlp_cifar10.pt')
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°epochç»“æœ
        print(f'Epoch {epoch:2d}/{epochs} | '
              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:6.2f}% | '
              f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:6.2f}% | '
              f'Best: {best_test_acc:6.2f}% | Time: {epoch_time:.1f}s')
        print("-" * 80)
    
    total_time = time.time() - total_start_time
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print("="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: best_mlp_cifar10.pt")
    print("="*60)
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    plot_learning_curves(train_losses, train_accs, test_losses, test_accs)
    
    # æœ€ç»ˆç»“æœæ‘˜è¦
    print("\nğŸ“Š æœ€ç»ˆç»“æœæ‘˜è¦:")
    print(f"  æ¨¡å‹æ¶æ„: 3å±‚MLP (3072â†’128â†’10)")
    print(f"  æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accs[-1]:.2f}%")
    print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accs[-1]:.2f}%")
    print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
    print(f"  è®­ç»ƒè½®æ¬¡: {epochs}")
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")

if __name__ == "__main__":
    main()
