'''
CIFAR-100 FULL HyperNetwork Implementation
ALL convolution and FC layers generated by hypernetworks

Enhanced Architecture:
- ALL 5 conv layers: hypernetwork generated
- ALL 3 FC layers: hypernetwork generated  
- Independent Z signals for each layer
- Advanced training with comprehensive augmentation
'''

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import time
import math

def set_device():
    """Set optimal device"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS (Metal Performance Shaders) acceleration")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("Using CUDA acceleration")
    else:
        device = torch.device("cpu")
        print("Using CPU")
    return device

class FullHyperCNNCIFAR(nn.Module):
    def __init__(self, z_dim=16):
        super(FullHyperCNNCIFAR, self).__init__()
        self.z_dim = z_dim
        
        # Network architecture
        self.channels = [3, 64, 128, 256, 512, 512]  # 5 conv layers
        self.f_sizes = [3, 3, 3, 3, 3]  # Kernel sizes
        self.fc_sizes = [512, 1024, 512, 100]  # FC layer sizes
        
        # Z signals for ALL layers (8 total: 5 conv + 3 fc)
        self.z_signal_conv1 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_conv2 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_conv3 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_conv4 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_conv5 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_fc1 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_fc2 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_fc3 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        
        # Hypernetwork generators for CONV layers
        conv1_weight_size = self.channels[1] * self.channels[0] * self.f_sizes[0] * self.f_sizes[0]
        self.hyper_conv1 = nn.Linear(z_dim, conv1_weight_size)
        self.conv1_bias = nn.Parameter(torch.zeros(self.channels[1]))
        
        conv2_weight_size = self.channels[2] * self.channels[1] * self.f_sizes[1] * self.f_sizes[1]
        self.hyper_conv2 = nn.Linear(z_dim, conv2_weight_size)
        self.conv2_bias = nn.Parameter(torch.zeros(self.channels[2]))
        
        conv3_weight_size = self.channels[3] * self.channels[2] * self.f_sizes[2] * self.f_sizes[2]
        self.hyper_conv3 = nn.Linear(z_dim, conv3_weight_size)
        self.conv3_bias = nn.Parameter(torch.zeros(self.channels[3]))
        
        conv4_weight_size = self.channels[4] * self.channels[3] * self.f_sizes[3] * self.f_sizes[3]
        self.hyper_conv4 = nn.Linear(z_dim, conv4_weight_size)
        self.conv4_bias = nn.Parameter(torch.zeros(self.channels[4]))
        
        conv5_weight_size = self.channels[5] * self.channels[4] * self.f_sizes[4] * self.f_sizes[4]
        self.hyper_conv5 = nn.Linear(z_dim, conv5_weight_size)
        self.conv5_bias = nn.Parameter(torch.zeros(self.channels[5]))
        
        # Hypernetwork generators for FC layers
        fc1_weight_size = self.fc_sizes[1] * self.fc_sizes[0]  # 1024 * 512
        self.hyper_fc1 = nn.Linear(z_dim, fc1_weight_size)
        self.fc1_bias = nn.Parameter(torch.zeros(self.fc_sizes[1]))
        
        fc2_weight_size = self.fc_sizes[2] * self.fc_sizes[1]  # 512 * 1024
        self.hyper_fc2 = nn.Linear(z_dim, fc2_weight_size)
        self.fc2_bias = nn.Parameter(torch.zeros(self.fc_sizes[2]))
        
        fc3_weight_size = self.fc_sizes[3] * self.fc_sizes[2]  # 100 * 512
        self.hyper_fc3 = nn.Linear(z_dim, fc3_weight_size)
        self.fc3_bias = nn.Parameter(torch.zeros(self.fc_sizes[3]))
        
        # BatchNorm layers (not generated by hypernetwork)
        self.bn1 = nn.BatchNorm2d(self.channels[1])
        self.bn2 = nn.BatchNorm2d(self.channels[2])
        self.bn3 = nn.BatchNorm2d(self.channels[3])
        self.bn4 = nn.BatchNorm2d(self.channels[4])
        self.bn5 = nn.BatchNorm2d(self.channels[5])
        
        # Pooling and dropout
        self.pool = nn.MaxPool2d(2, 2)
        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.5)
        
        # Initialize hypernetworks
        self._init_hypernetworks()
        
        print(f"FULL HyperNetwork Architecture:")
        print(f"  Channels: {self.channels}")
        print(f"  FC sizes: {self.fc_sizes}")
        print(f"  Conv1 hypernetwork: {z_dim} -> {conv1_weight_size} parameters")
        print(f"  Conv2 hypernetwork: {z_dim} -> {conv2_weight_size} parameters")
        print(f"  Conv3 hypernetwork: {z_dim} -> {conv3_weight_size} parameters")
        print(f"  Conv4 hypernetwork: {z_dim} -> {conv4_weight_size} parameters")
        print(f"  Conv5 hypernetwork: {z_dim} -> {conv5_weight_size} parameters")
        print(f"  FC1 hypernetwork: {z_dim} -> {fc1_weight_size} parameters")
        print(f"  FC2 hypernetwork: {z_dim} -> {fc2_weight_size} parameters")
        print(f"  FC3 hypernetwork: {z_dim} -> {fc3_weight_size} parameters")
        
    def _init_hypernetworks(self):
        """Initialize all hypernetwork components with small weights"""
        hypernetworks = [
            self.hyper_conv1, self.hyper_conv2, self.hyper_conv3, 
            self.hyper_conv4, self.hyper_conv5,
            self.hyper_fc1, self.hyper_fc2, self.hyper_fc3
        ]
        
        for hyper_net in hypernetworks:
            nn.init.normal_(hyper_net.weight, std=0.01)
            nn.init.constant_(hyper_net.bias, 0.0)
        
        # Initialize BatchNorm
        for m in self.modules():
            if isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def generate_conv1_weights(self):
        """Generate conv1 weights using hypernetwork"""
        weights_flat = self.hyper_conv1(self.z_signal_conv1)
        return weights_flat.reshape(self.channels[1], self.channels[0], self.f_sizes[0], self.f_sizes[0])
    
    def generate_conv2_weights(self):
        """Generate conv2 weights using hypernetwork"""
        weights_flat = self.hyper_conv2(self.z_signal_conv2)
        return weights_flat.reshape(self.channels[2], self.channels[1], self.f_sizes[1], self.f_sizes[1])
    
    def generate_conv3_weights(self):
        """Generate conv3 weights using hypernetwork"""
        weights_flat = self.hyper_conv3(self.z_signal_conv3)
        return weights_flat.reshape(self.channels[3], self.channels[2], self.f_sizes[2], self.f_sizes[2])
    
    def generate_conv4_weights(self):
        """Generate conv4 weights using hypernetwork"""
        weights_flat = self.hyper_conv4(self.z_signal_conv4)
        return weights_flat.reshape(self.channels[4], self.channels[3], self.f_sizes[3], self.f_sizes[3])
    
    def generate_conv5_weights(self):
        """Generate conv5 weights using hypernetwork"""
        weights_flat = self.hyper_conv5(self.z_signal_conv5)
        return weights_flat.reshape(self.channels[5], self.channels[4], self.f_sizes[4], self.f_sizes[4])
    
    def generate_fc1_weights(self):
        """Generate fc1 weights using hypernetwork"""
        weights_flat = self.hyper_fc1(self.z_signal_fc1)
        return weights_flat.reshape(self.fc_sizes[1], self.fc_sizes[0])
    
    def generate_fc2_weights(self):
        """Generate fc2 weights using hypernetwork"""
        weights_flat = self.hyper_fc2(self.z_signal_fc2)
        return weights_flat.reshape(self.fc_sizes[2], self.fc_sizes[1])
    
    def generate_fc3_weights(self):
        """Generate fc3 weights using hypernetwork"""
        weights_flat = self.hyper_fc3(self.z_signal_fc3)
        return weights_flat.reshape(self.fc_sizes[3], self.fc_sizes[2])
        
    def forward(self, x):
        # Conv1 block (hypernetwork generated)
        conv1_weights = self.generate_conv1_weights()
        x = F.conv2d(x, conv1_weights, bias=self.conv1_bias, padding=1)
        x = self.pool(F.relu(self.bn1(x)))  # 32->16
        
        # Conv2 block (hypernetwork generated)
        conv2_weights = self.generate_conv2_weights()
        x = F.conv2d(x, conv2_weights, bias=self.conv2_bias, padding=1)
        x = self.pool(F.relu(self.bn2(x)))  # 16->8
        
        # Conv3 block (hypernetwork generated)
        conv3_weights = self.generate_conv3_weights()
        x = F.conv2d(x, conv3_weights, bias=self.conv3_bias, padding=1)
        x = self.pool(F.relu(self.bn3(x)))  # 8->4
        
        # Conv4 block (hypernetwork generated)
        conv4_weights = self.generate_conv4_weights()
        x = F.conv2d(x, conv4_weights, bias=self.conv4_bias, padding=1)
        x = self.pool(F.relu(self.bn4(x)))  # 4->2
        
        # Conv5 block (hypernetwork generated)
        conv5_weights = self.generate_conv5_weights()
        x = F.conv2d(x, conv5_weights, bias=self.conv5_bias, padding=1)
        x = F.relu(self.bn5(x))  # 2x2
        
        # Global average pooling
        x = self.adaptive_pool(x)  # 2x2 -> 1x1
        x = x.view(x.size(0), -1)  # Flatten
        
        # FC1 block (hypernetwork generated)
        fc1_weights = self.generate_fc1_weights()
        x = F.linear(x, fc1_weights, bias=self.fc1_bias)
        x = F.relu(x)
        x = self.dropout(x)
        
        # FC2 block (hypernetwork generated)
        fc2_weights = self.generate_fc2_weights()
        x = F.linear(x, fc2_weights, bias=self.fc2_bias)
        x = F.relu(x)
        x = self.dropout(x)
        
        # FC3 block (hypernetwork generated)
        fc3_weights = self.generate_fc3_weights()
        x = F.linear(x, fc3_weights, bias=self.fc3_bias)
        
        return x

def get_data_loaders(batch_size=64, num_workers=4):
    """Enhanced data augmentation for better generalization"""
    
    # Strong data augmentation for training
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),
        transforms.RandomErasing(p=0.1)  # Cutout augmentation
    ])
    
    # Simple normalization for validation/test
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])
    
    # Load datasets
    train_dataset = datasets.CIFAR100('data', train=True, download=False, transform=transform_train)
    test_dataset = datasets.CIFAR100('data', train=False, transform=transform_test)
    
    # Create validation split
    train_size = len(train_dataset) - 5000
    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, 5000])
    
    # Apply test transform to validation set
    val_dataset.dataset = datasets.CIFAR100('data', train=True, download=False, transform=transform_test)
    val_indices = val_dataset.indices
    val_dataset = torch.utils.data.Subset(val_dataset.dataset, val_indices)
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=num_workers, pin_memory=True, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size*2, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size*2, num_workers=num_workers, pin_memory=True)
    
    return train_loader, val_loader, test_loader

def train_epoch(model, device, train_loader, optimizer, epoch, log_interval=100):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # Track metrics
        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)
        
        if batch_idx % log_interval == 0 and batch_idx > 0:
            current_acc = 100. * correct / total
            print(f'  Batch: {batch_idx:3d}/{len(train_loader)} | '
                  f'Loss: {loss.item():.4f} | '
                  f'Acc: {current_acc:6.2f}%')
            
            # Debug hypernetwork signals - sample a few
            conv_norms = [
                model.z_signal_conv1.norm().item(),
                model.z_signal_conv2.norm().item(),
                model.z_signal_conv3.norm().item()
            ]
            fc_norms = [
                model.z_signal_fc1.norm().item(),
                model.z_signal_fc2.norm().item(),
                model.z_signal_fc3.norm().item()
            ]
            print(f'    Conv Z-norms: {conv_norms[0]:.4f}, {conv_norms[1]:.4f}, {conv_norms[2]:.4f}')
            print(f'    FC Z-norms: {fc_norms[0]:.4f}, {fc_norms[1]:.4f}, {fc_norms[2]:.4f}')
    
    train_loss /= len(train_loader)
    train_acc = correct / total
    return train_loss, 1.0 - train_acc

def evaluate(model, device, data_loader):
    model.eval()
    loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in data_loader:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            output = model(data)
            loss += F.cross_entropy(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    
    loss /= total
    acc = correct / total
    return loss, 1.0 - acc

def count_parameters(model):
    """Count and categorize parameters"""
    total_params = 0
    hyper_params = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            num_params = param.numel()
            total_params += num_params
            
            if 'z_signal' in name or 'hyper_' in name:
                hyper_params += num_params
                print(f"[HYPER] {name}: {param.shape}, {num_params:,}")
            else:
                print(f"[OTHER] {name}: {param.shape}, {num_params:,}")
    
    print(f"\nTotal trainable parameters: {total_params:,}")
    print(f"Hypernetwork parameters: {hyper_params:,}")
    print(f"Standard parameters: {total_params - hyper_params:,}")
    print(f"Hypernetwork ratio: {100.*hyper_params/total_params:.2f}%")
    return total_params, hyper_params

def cosine_annealing_lr(optimizer, epoch, total_epochs, max_lr, min_lr=1e-6):
    """Cosine annealing learning rate schedule"""
    lr = min_lr + (max_lr - min_lr) * (1 + math.cos(math.pi * epoch / total_epochs)) / 2
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr

def main():
    print("Starting CIFAR-100 FULL HyperNetwork Training...")
    print("ALL layers (Conv + FC) generated by hypernetworks!")
    print("TARGET: 90% Test Accuracy")
    print("="*80)
    
    # Set seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    
    device = set_device()
    
    # Enhanced hyperparameters
    batch_size = 64          # Smaller batch for better gradients
    epochs = 100             # Much longer training
    max_lr = 0.001           # Initial learning rate
    min_lr = 1e-6            # Minimum learning rate
    z_dim = 16               # Z signal dimension
    weight_decay = 1e-4      # L2 regularization
    
    print(f"Full HyperNetwork Hyperparameters:")
    print(f"  Batch size: {batch_size}")
    print(f"  Epochs: {epochs}")
    print(f"  Max learning rate: {max_lr}")
    print(f"  Z dimension: {z_dim}")
    print(f"  Weight decay: {weight_decay}")
    print("="*80)
    
    # Load data with enhanced augmentation
    print("Loading CIFAR-100 with enhanced data augmentation...")
    train_loader, val_loader, test_loader = get_data_loaders(batch_size)
    
    print(f"Dataset sizes: Train={len(train_loader.dataset)}, "
          f"Val={len(val_loader.dataset)}, Test={len(test_loader.dataset)}")
    print("="*80)
    
    # Create FULL hypernetwork model
    print("Creating FULL HyperNetwork (ALL layers generated)...")
    model = FullHyperCNNCIFAR(z_dim=z_dim).to(device)
    
    # Advanced optimizer
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay, betas=(0.9, 0.999))
    
    print("\nFull HyperNetwork parameters:")
    total_params, hyper_params = count_parameters(model)
    print("="*80)
    
    # Training variables
    best_val_acc = 0.0
    best_test_acc = 0.0
    patience = 15
    patience_counter = 0
    
    print(f"Starting training for {epochs} epochs...")
    print(f"Target: 90% test accuracy")
    print("="*80)
    
    for epoch in range(epochs):
        start_time = time.time()
        
        # Cosine annealing learning rate
        current_lr = cosine_annealing_lr(optimizer, epoch, epochs, max_lr, min_lr)
        
        print(f"\nEpoch {epoch+1}/{epochs} - LR: {current_lr:.6f}")
        print("-" * 60)
        
        # Training
        train_loss, train_err = train_epoch(model, device, train_loader, optimizer, epoch)
        
        # Validation
        val_loss, val_err = evaluate(model, device, val_loader)
        val_acc = 1.0 - val_err
        
        # Test evaluation (every 5 epochs or if validation improved)
        test_acc = 0.0
        if epoch % 5 == 0 or val_acc > best_val_acc:
            test_loss, test_err = evaluate(model, device, test_loader)
            test_acc = 1.0 - test_err
            
            if test_acc > best_test_acc:
                best_test_acc = test_acc
                torch.save(model.state_dict(), 'cifar100_full_hyper_best.pt')
                print(f"    *** New best test accuracy: {best_test_acc*100:.2f}% - Model saved! ***")
        
        # Early stopping based on validation
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
        else:
            patience_counter += 1
        
        epoch_time = time.time() - start_time
        
        # Print epoch results
        print(f"\nEpoch Results:")
        print(f"  Train: Loss={train_loss:.4f}, Acc={100*(1-train_err):.2f}%")
        print(f"  Val:   Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%")
        print(f"  Test:  Acc={test_acc*100:.2f}% (Best: {best_test_acc*100:.2f}%)")
        print(f"  Time: {epoch_time:.1f}s, Patience: {patience_counter}/{patience}")
        
        # Check if we reached target
        if best_test_acc >= 0.90:
            print(f"\n🎉 TARGET ACHIEVED! Best test accuracy: {best_test_acc*100:.2f}% >= 90%")
            break
            
        # Early stopping
        if patience_counter >= patience:
            print(f"\nEarly stopping triggered after {patience} epochs without improvement")
            break
    
    # Final evaluation
    print("\n" + "="*80)
    print("FINAL RESULTS")
    print("="*80)
    
    # Load best model
    model.load_state_dict(torch.load('cifar100_full_hyper_best.pt'))
    final_test_loss, final_test_err = evaluate(model, device, test_loader)
    final_test_acc = (1 - final_test_err) * 100
    
    print(f"FULL HyperNetwork - Final Test Accuracy: {final_test_acc:.2f}%")
    print(f"Enhanced HyperNetwork (partial) - Test Accuracy: 63.44%")
    print(f"Fixed HyperNetwork - Test Accuracy: 33.52%")
    print(f"Standard CNN - Test Accuracy: 16.23%")
    
    improvement_vs_enhanced = final_test_acc - 63.44
    improvement_vs_fixed = final_test_acc - 33.52
    improvement_vs_std = final_test_acc - 16.23
    
    print(f"Improvement vs Enhanced HyperNet: {improvement_vs_enhanced:+.2f}%")
    print(f"Improvement vs Fixed HyperNet: {improvement_vs_fixed:+.2f}%")
    print(f"Improvement vs Standard CNN: {improvement_vs_std:+.2f}%")
    
    # Print final z_signals (sample a few)
    print(f"\nFinal hypernetwork signals (sample):")
    print(f"z_signal_conv1: {model.z_signal_conv1.detach().cpu().numpy().flatten()}")
    print(f"z_signal_conv3: {model.z_signal_conv3.detach().cpu().numpy().flatten()}")
    print(f"z_signal_fc1: {model.z_signal_fc1.detach().cpu().numpy().flatten()}")
    print(f"z_signal_fc3: {model.z_signal_fc3.detach().cpu().numpy().flatten()}")
    
    # Model size
    model_size_mb = total_params * 4 / (1024 * 1024)  # Assume float32
    print(f"\nModel statistics:")
    print(f"  Total parameters: {total_params:,}")
    print(f"  Estimated size: {model_size_mb:.1f} MB")
    
    target_reached = "YES ✅" if final_test_acc >= 90.0 else "NO ❌"
    print(f"\nTarget (90% accuracy) reached: {target_reached}")
    
    print("\nTraining completed!")

if __name__ == "__main__":
    main()
