'''
MNIST Static HyperNetwork Example
Converted from MNIST_Static_HyperNetwork_Example.ipynb

This script demonstrates the concept of static hypernetworks on MNIST dataset.
It trains two models:
1. A baseline CNN model
2. A hypernetwork-based CNN model where weights are generated by a smaller network

Based on the paper "Hypernetworks" by David Ha, Andrew Dai, and Quoc V. Le.
'''

import numpy as np
import time
import random
try:
    import cPickle  # Python 2
except ImportError:
    import pickle as cPickle  # Python 3
import codecs
import collections
import os
import tensorflow as tf
import tensorflow.compat.v1 as tf1
tf1.disable_eager_execution()  # Use TensorFlow 1.x compatibility mode
# Modern way to load MNIST data
try:
    # Try standalone keras package
    import keras
    from keras.datasets import mnist
    use_keras_mnist = True
    print("Using standalone Keras for MNIST data")
except ImportError:
    # Fallback to scikit-learn's fetch_openml for MNIST
    try:
        from sklearn.datasets import fetch_openml
        use_keras_mnist = False
        print("Using scikit-learn for MNIST data")
    except ImportError:
        raise ImportError("Neither keras nor scikit-learn is available for loading MNIST")

try:
    from PIL import Image
except ImportError:
    import PIL
    from PIL import Image
import matplotlib
import matplotlib.pyplot as plt

# Configure matplotlib for non-interactive backend if running as script
matplotlib.use('Agg')

# misc
np.set_printoptions(precision=5, edgeitems=8, linewidth=200)

# Utility Functions
def orthogonal(shape):
    flat_shape = (shape[0], np.prod(shape[1:]))
    a = np.random.normal(0.0, 1.0, flat_shape)
    u, _, v = np.linalg.svd(a, full_matrices=False)
    q = u if u.shape == flat_shape else v
    return q.reshape(shape)

def orthogonal_initializer(scale=1.0):
    def _initializer(shape, dtype=tf.float32, partition_info=None):
        return tf.constant(orthogonal(shape) * scale, dtype)
    return _initializer

def super_linear(x, output_size, scope=None, reuse=False, init_w="ortho", weight_start=0.0, use_bias=True, bias_start=0.0):
    # support function doing linear operation.  uses ortho initializer defined earlier.
    shape = x.get_shape().as_list()
    with tf1.variable_scope(scope or "linear"):
        if reuse == True:
            tf1.get_variable_scope().reuse_variables()

        w_init = None # uniform
        x_size = shape[1]
        h_size = output_size
        if init_w == "zeros":
            w_init=tf1.constant_initializer(0.0)
        elif init_w == "constant":
            w_init=tf1.constant_initializer(weight_start)
        elif init_w == "gaussian":
            w_init=tf1.random_normal_initializer(stddev=weight_start)
        elif init_w == "ortho":
            w_init=orthogonal_initializer(1.0)

        w = tf1.get_variable("super_linear_w",
            [shape[1], output_size], tf.float32, initializer=w_init)
        if use_bias:
            b = tf1.get_variable("super_linear_b", [output_size], tf.float32,
                initializer=tf1.constant_initializer(bias_start))
            return tf.matmul(x, w) + b
        return tf.matmul(x, w)

# Dataset class for MNIST
class DataSet(object):
    def __init__(self, images, labels, augment=False):
        # Convert from [0, 255] -> [0.0, 1.0]
        images = images.astype(np.float32)
        self.image_size = 28
        self._num_examples = len(images)
        images = np.reshape(images, (self._num_examples, self.image_size, self.image_size, 1))
        perm = np.arange(self._num_examples)
        np.random.shuffle(perm)
        self._images = images[perm]
        self._labels = labels[perm]
        self._augment = augment
        self.pointer = 0
        self.upsize = 1 if self._augment else 0
        self.min_upsize = 2
        self.max_upsize = 2
        self.random_perm_mode=False
        self.num_classes = 10

    @property
    def images(self):
        return self._images
    
    @property
    def labels(self):
        return self._labels
    
    @property
    def num_examples(self):
        return self._num_examples

    def next_batch(self, batch_size=100, with_label = True, one_hot = False):
        if self.pointer >= self.num_examples-2*batch_size:
            self.pointer = 0
        else:
            self.pointer += batch_size
        result = []
        
        upsize_amount = np.random.randint(self.upsize*self.min_upsize, self.upsize*self.max_upsize+1)
        
        def upsize_row_once(img):
            old_size = img.shape[0]
            new_size = old_size+1
            new_img = np.zeros((new_size, img.shape[1], 1))
            rand_row = np.random.randint(1, old_size-1)
            new_img[0:rand_row,:] = img[0:rand_row,:]
            new_img[rand_row+1:,:] = img[rand_row:,:]
            new_img[rand_row,:] = 0.5*(new_img[rand_row-1,:]+new_img[rand_row+1,:])
            return new_img
        
        def upsize_col_once(img):
            old_size = img.shape[1]
            new_size = old_size+1
            new_img = np.zeros((img.shape[0], new_size, 1))
            rand_col = np.random.randint(1, old_size-1)
            new_img[:,0:rand_col,:] = img[:,0:rand_col,:]
            new_img[:,rand_col+1:,:] = img[:,rand_col:,:]
            new_img[:,rand_col,:] = 0.5*(new_img[:,rand_col-1,:]+new_img[:,rand_col+1,:])
            return new_img
        
        def upsize_me(img, n=2):
            new_img = img
            for i in range(n):
                new_img = upsize_row_once(new_img)
                new_img = upsize_col_once(new_img)
            return new_img

        for data in self._images[self.pointer:self.pointer+batch_size]:
            result.append(self.distort_image(upsize_me(data, upsize_amount), upsize_amount))
            
        if len(result) != batch_size:
            print("uh oh, self.pointer = ", self.pointer)
        assert(len(result) == batch_size)
        result_labels = self.labels[self.pointer:self.pointer+batch_size]
        assert(len(result_labels) == batch_size)
        
        if one_hot:
            result_labels = np.eye(self.num_classes)[result_labels]
            
        if with_label:
            return self.scramble_batch(np.array(result, dtype=np.float32)), result_labels
        return self.scramble_batch(np.array(result, dtype=np.float32))

    def distort_batch(self, batch, upsize_amount):
        batch_size = len(batch)
        row_distort = np.random.randint(0, self.image_size+upsize_amount-self.image_size+1, batch_size)
        col_distort = np.random.randint(0, self.image_size+upsize_amount-self.image_size+1, batch_size)
        result = np.zeros(shape=(batch_size, self.image_size, self.image_size, 1), dtype=np.float32)
        for i in range(batch_size):
            result[i, :, :, :] = batch[i, row_distort[i]:row_distort[i]+self.image_size, col_distort[i]:col_distort[i]+self.image_size, :]
        return result
    
    def scramble_batch(self, batch):
        if self.random_perm_mode:
            batch_size = len(batch)
            result = np.copy(batch)
            result = result.reshape(batch_size, self.image_size*self.image_size)
            result = result[:, self.random_key]
            return result
        else:
            result = batch
            return result
    
    def distort_image(self, img, upsize_amount):
        row_distort = np.random.randint(0, self.image_size+upsize_amount-self.image_size+1)
        col_distort = np.random.randint(0, self.image_size+upsize_amount-self.image_size+1)
        result = np.zeros(shape=(self.image_size, self.image_size, 1), dtype=np.float32)
        result[:, :, :] = img[row_distort:row_distort+self.image_size, col_distort:col_distort+self.image_size, :]
        return result

    def shuffle_data(self):
        perm = np.arange(self._num_examples)
        np.random.shuffle(perm)
        self._images = self._images[perm]
        self._labels = self._labels[perm]

# Utility functions for visualization and data handling
def show_image(image):
    plt.subplot(1, 1, 1)
    plt.imshow(np.reshape(image, (28, 28)), cmap='Greys', interpolation='nearest')
    plt.axis('off')
    plt.savefig("mnist_image.png")
    plt.close()

def show_filter(w_orig, save_path="filter_visualization.png"):
    w = w_orig.T
    the_shape = w_orig.shape
    print(the_shape)
    f_size = the_shape[0]
    in_dim = the_shape[2]
    out_dim = the_shape[3]
    print("mean =", np.mean(w))
    print("stddev =", np.std(w))
    print("max =", np.max(w))
    print("min =", np.min(w))
    print("median =", np.median(w))
    canvas = np.zeros(((f_size+1)*out_dim, (f_size+1)*in_dim))
    for i in range(out_dim):
        for j in range(in_dim):
            canvas[i*(f_size+1):i*(f_size+1)+f_size,j*(f_size+1):j*(f_size+1)+f_size] = w[i, j]
    plt.figure(figsize=(16, 16))
    canvas_fixed = np.zeros((canvas.shape[0]+1,canvas.shape[1]+1))
    canvas_fixed[1:,1:] = canvas
    plt.imshow(canvas_fixed.T, cmap='Greys', interpolation='nearest')
    plt.axis('off')
    plt.savefig(save_path)
    plt.close()

def read_data_sets(mnist_data):
    class DataSets(object):
        pass
    data_sets = DataSets()

    data_sets.train = DataSet(mnist_data.train.images, mnist_data.train.labels, augment=True)
    data_sets.valid = DataSet(mnist_data.validation.images, mnist_data.validation.labels, augment=False)
    data_sets.test = DataSet(mnist_data.test.images, mnist_data.test.labels, augment=False)
    XDIM = data_sets.train.image_size
    return data_sets

# MNIST model class
class MNIST(object):
    def __init__(self, hps_model, reuse=False, gpu_mode=True, is_training=True):
        self.is_training = is_training
        with tf1.variable_scope('conv_mnist', reuse=reuse):
            if not gpu_mode:
                with tf1.device("/cpu:0"):
                    print("model using cpu")
                    self.build_model(hps_model)
            else:
                self.build_model(hps_model)

    def build_model(self, hps_model):
        self.hps = hps_model
        
        self.model_path = self.hps.model_path
        self.model_save_path = self.model_path + 'mnist'
        
        self.batch_images = tf1.placeholder(tf.float32, [self.hps.batch_size, self.hps.x_dim, self.hps.x_dim, self.hps.c_dim])
        self.batch_labels = tf1.placeholder(tf.float32, [self.hps.batch_size, self.hps.num_classes]) # one-hot labels.
        
        '''
        settings for architecture:
        '''
        f_size=7
        in_size=16
        out_size=16
        z_dim=4

        conv1_weights = tf1.Variable(tf1.truncated_normal([f_size, f_size, 1, out_size], stddev=0.01), name="conv1_weights")
        
        if self.hps.hyper_mode:
            # the static hypernetwork is inside this if statement.
            w1 = tf1.get_variable('w1',[z_dim, out_size*f_size*f_size],initializer=tf1.truncated_normal_initializer(stddev=0.01))
            b1 = tf1.get_variable('b1', [out_size*f_size*f_size], initializer=tf1.constant_initializer(0.0))
            z2 = tf1.get_variable("z_signal_2", [1, z_dim], tf.float32, initializer=tf1.truncated_normal_initializer(0.01))
            w2 = tf1.get_variable('w2',[z_dim, in_size*z_dim],initializer=tf1.truncated_normal_initializer(stddev=0.01))
            b2 = tf1.get_variable('b2', [in_size*z_dim], initializer=tf1.constant_initializer(0.0))
            h_in = tf.matmul(z2, w2) + b2
            h_in = tf.reshape(h_in, [in_size, z_dim])
            h_final = tf.matmul(h_in, w1) + b1
            kernel2 = tf.reshape(h_final, (out_size, in_size, f_size, f_size))
            conv2_weights = tf.transpose(kernel2)
        else:
            conv2_weights = tf1.Variable(tf1.truncated_normal([f_size, f_size, in_size, out_size], stddev=0.01), name="conv2_weights")

        self.conv1_weights = conv1_weights
        self.conv2_weights = conv2_weights

        conv1_biases = tf1.Variable(tf1.zeros([in_size]), name="conv1_biases")

        net = tf.nn.conv2d(self.batch_images, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')
        net = tf.nn.relu(net + conv1_biases)
        net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')

        conv2_biases = tf1.Variable(tf1.zeros([out_size]), name="conv2_biases")

        net = tf.nn.conv2d(net, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')
        net = tf.nn.relu(net + conv2_biases)
        net = tf.nn.max_pool(net, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')
        
        # Reshapes the hidden units such that instead of 2D maps, they are 1D vectors:
        net = tf.reshape(net, [self.hps.batch_size, -1])
        
        net = super_linear(net, self.hps.num_classes, scope='fc_final')
        
        self.logits = net
        self.probabilities = tf.nn.softmax(self.logits)
        self.predictions = tf.argmax(self.logits, 1)

        # Specify the loss function:
        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.batch_labels)
        self.loss = tf.reduce_mean(cross_entropy)
        
        # Specify the optimization scheme:
        self.lr = tf1.Variable(self.hps.lr, trainable=False)
        optimizer = tf1.train.AdamOptimizer(self.lr)
        tvars = tf1.trainable_variables()
        grads, _ = tf1.clip_by_global_norm(tf1.gradients(self.loss, tvars),
                self.hps.grad_clip)
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))

        # model saver
        self.saver = tf1.train.Saver(tf1.global_variables())
        
    def update_lr(self, sess):
        lr = sess.run(self.lr)
        lr *= self.hps.lr_decay
        sess.run(tf1.assign(self.lr, np.maximum(lr, self.hps.min_lr)))
        
    def partial_train(self, sess, batch_images, batch_labels):
        _, loss, pred, lr = sess.run((self.train_op, self.loss, self.predictions, self.lr),
                                    feed_dict={self.batch_images: batch_images, self.batch_labels: batch_labels})
        return loss, pred, lr
    
    def partial_eval(self, sess, batch_images, batch_labels):
        loss, pred = sess.run((self.loss, self.predictions),
                             feed_dict={self.batch_images: batch_images, self.batch_labels: batch_labels})
        return loss, pred
    
    def save_model(self, sess, epoch = 0):
        checkpoint_path=self.model_save_path
        print("saving model: ", checkpoint_path)
        self.saver.save(sess, checkpoint_path, global_step = epoch)
        
    def load_model(self, sess):
        checkpoint_path = self.model_path
        ckpt = tf.train.get_checkpoint_state(checkpoint_path)
        print("loading model: ", ckpt.model_checkpoint_path)
        self.saver.restore(sess, ckpt.model_checkpoint_path)

# Training functions
def process_epoch(sess, model, dataset, train_mode=False, print_every=0):
    num_examples = dataset.num_examples
    batch_size = hps_model.batch_size
    total_batch = int(num_examples / batch_size)
    
    avg_loss = 0.
    avg_pred_error = 0.
    lr = model.hps.lr

    for i in range(total_batch):
        batch_images, batch_labels = dataset.next_batch(batch_size, with_label=True, one_hot=False)

        if train_mode:
            loss, pred, lr = model.partial_train(sess, batch_images, np.eye(dataset.num_classes)[batch_labels])
            model.update_lr(sess)
        else:
            loss, pred = model.partial_eval(sess, batch_images, np.eye(dataset.num_classes)[batch_labels])

        pred_error = 1.0 - np.sum((pred == batch_labels)) / float(batch_size)
        
        if print_every > 0 and i > 0 and i % print_every == 0:
            print("Batch:", '%d' % (i), \
                "/", '%d' % (total_batch), \
                "loss=", "{:.4f}".format(loss), \
                "err=", "{:.4f}".format(pred_error))
                
        assert(loss < 1000000) # make sure it is not NaN or Inf

        avg_loss += loss / num_examples * batch_size
        avg_pred_error += pred_error / num_examples * batch_size
    
    return avg_loss, avg_pred_error, lr

def train_model(sess, model, eval_model, mnist, num_epochs, save_model=True):
    # train the model for num_epochs
    best_valid_loss = 100.
    best_valid_pred_error = 1.0
    eval_loss = 100.
    eval_pred_error = 1.0
    
    for epoch in range(num_epochs):
        train_loss, train_pred_error, lr = process_epoch(sess, model, mnist.train, train_mode=True, print_every=10)
        
        valid_loss, valid_pred_error, _ = process_epoch(sess, eval_model, mnist.valid, train_mode=False)
        
        if valid_pred_error <= best_valid_pred_error:
            best_valid_pred_error = valid_pred_error
            best_valid_loss = valid_loss
            eval_loss, eval_pred_error, _ = process_epoch(sess, eval_model, mnist.test, train_mode=False)
            
            if save_model:
                model.save_model(sess, epoch)

        print("Epoch:", '%d' % (epoch), \
            "train_loss=", "{:.4f}".format(train_loss), \
            "train_err=", "{:.4f}".format(train_pred_error), \
            "valid_err=", "{:.4f}".format(valid_pred_error), \
            "best_valid_err=", "{:.4f}".format(best_valid_pred_error), \
            "test_err=", "{:.4f}".format(eval_pred_error), \
            "lr=", "{:.6f}".format(lr))

def load_mnist_keras():
    """Load MNIST data using Keras datasets API"""
    print("Loading MNIST data using Keras...")
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    
    # Normalize to [0, 1]
    x_train = x_train.astype(np.float32) / 255.0
    x_test = x_test.astype(np.float32) / 255.0
    
    # Split test into validation and test
    val_size = 10000
    x_val = x_test[:val_size]
    y_val = y_test[:val_size]
    x_test = x_test[val_size:]
    y_test = y_test[val_size:]
    
    # Create a struct-like object to match the old API
    class MNISTData:
        class DataSet:
            def __init__(self, images, labels):
                self.images = images
                self.labels = labels
        
        def __init__(self, train, validation, test):
            self.train = train
            self.validation = validation
            self.test = test
    
    train_dataset = MNISTData.DataSet(x_train, y_train)
    validation_dataset = MNISTData.DataSet(x_val, y_val)
    test_dataset = MNISTData.DataSet(x_test, y_test)
    
    return MNISTData(train_dataset, validation_dataset, test_dataset)

def load_mnist_sklearn():
    """Load MNIST data using scikit-learn"""
    print("Loading MNIST data using scikit-learn...")
    # Fetch MNIST data from OpenML
    X, y = fetch_openml('mnist_784', version=1, return_X_y=True)
    
    # Normalize to [0, 1]
    X = X.astype(np.float32) / 255.0
    y = y.astype(np.int64)
    
    # Split into train, validation and test sets
    X_train, X_test = X[:60000], X[60000:]
    y_train, y_test = y[:60000], y[60000:]
    
    X_test, X_val = X_test[:10000], X_test[10000:]
    y_test, y_val = y_test[:10000], y_test[10000:]
    
    # Create a struct-like object to match the old API
    class MNISTData:
        class DataSet:
            def __init__(self, images, labels):
                self.images = images
                self.labels = labels
        
        def __init__(self, train, validation, test):
            self.train = train
            self.validation = validation
            self.test = test
    
    train_dataset = MNISTData.DataSet(X_train, y_train)
    validation_dataset = MNISTData.DataSet(X_val, y_val)
    test_dataset = MNISTData.DataSet(X_test, y_test)
    
    return MNISTData(train_dataset, validation_dataset, test_dataset)

def main():
    print("Starting MNIST Hypernetwork Example...")
    
    # Hyperparameters
    class HParams(object):
        pass

    global hps_model
    hps_model = HParams()
    hps_model.lr=0.005
    hps_model.lr_decay=0.999
    hps_model.min_lr=0.0001
    hps_model.is_training=True
    hps_model.x_dim=28
    hps_model.num_classes=10
    hps_model.c_dim=1
    hps_model.batch_size=1000  # Match original notebook batch size
    hps_model.grad_clip=100.0
    hps_model.hyper_mode=False
    hps_model.model_path='/tmp/'
    
    # Create directory if it doesn't exist
    os.makedirs(hps_model.model_path, exist_ok=True)
    
    # Load MNIST data using the appropriate method
    print("Loading MNIST data...")
    if use_keras_mnist:
        mnist_data = load_mnist_keras()
    else:
        mnist_data = load_mnist_sklearn()
        
    mnist = read_data_sets(mnist_data)
    
    # Train standard CNN model
    print("\nTraining standard CNN model...")
    tf1.reset_default_graph()
    with tf1.Session() as sess:
        model = MNIST(hps_model)
        sess.run(tf1.global_variables_initializer())
        
        # Train for 50 epochs as in the original notebook
        train_model(sess, model, model, mnist, 50, save_model=False)
        
        # Get convolutional filter visualization
        conv_filter = sess.run(model.conv2_weights)
        show_filter(conv_filter, "standard_cnn_filter.png")
        
        # Count parameters
        t_vars = tf1.trainable_variables()
        count_t_vars = 0
        for var in t_vars:
            num_param = np.prod(var.get_shape().as_list())
            count_t_vars += num_param
            print(var.name, var.get_shape(), num_param)
        print("Total trainable variables = %d" % (count_t_vars))
    
    # Train hypernetwork CNN model
    print("\nTraining hypernetwork CNN model...")
    hps_model.hyper_mode = True
    tf1.reset_default_graph()
    with tf1.Session() as sess:
        model = MNIST(hps_model)
        sess.run(tf1.global_variables_initializer())
        
        # Train for 50 epochs as in the original notebook
        train_model(sess, model, model, mnist, 50, save_model=False)
        
        # Get hypernetwork-generated convolutional filter visualization
        conv_filter = sess.run(model.conv2_weights)
        show_filter(conv_filter, "hypernetwork_cnn_filter.png")
        
        # Count parameters
        t_vars = tf1.trainable_variables()
        count_t_vars = 0
        for var in t_vars:
            num_param = np.prod(var.get_shape().as_list())
            count_t_vars += num_param
            print(var.name, var.get_shape(), num_param)
        print("Total trainable variables = %d" % (count_t_vars))
    
    print("\nExecution complete. Filter visualizations saved as PNG files.")

if __name__ == "__main__":
    main()
