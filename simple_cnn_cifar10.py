"""
Simple Small-Scale CNN for CIFAR-10 Classification
å°†simple_mlp_cifar10.pyä¸­çš„MLPæ›¿æ¢ä¸ºå°è§„æ¨¡CNNè¿›è¡Œå¯¹æ¯”å®éªŒ

Architecture:
- Conv1: 3â†’32, 3Ã—3, padding=1
- MaxPool: 2Ã—2  
- Conv2: 32â†’64, 3Ã—3, padding=1
- MaxPool: 2Ã—2
- Conv3: 64â†’128, 3Ã—3, padding=1
- MaxPool: 2Ã—2
- FC1: 128Ã—4Ã—4â†’128
- FC2: 128â†’10

Hardware: MacBook Pro M4 Pro with MPS acceleration
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import matplotlib
import numpy as np
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def set_device():
    """è®¾ç½®M4 Proçš„æœ€ä¼˜è®¾å¤‡"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… ä½¿ç”¨MPS (Metal Performance Shaders) åŠ é€Ÿ - M4 Pro")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("âœ… ä½¿ç”¨CUDAåŠ é€Ÿ")
    else:
        device = torch.device("cpu")
        print("âš ï¸  ä½¿ç”¨CPU")
    return device

class SimpleCNN(nn.Module):
    """å°è§„æ¨¡CNNç½‘ç»œ"""
    def __init__(self, num_classes=10):
        super(SimpleCNN, self).__init__()
        
        # å·ç§¯å±‚åºåˆ—
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # 32x32x32
        self.pool1 = nn.MaxPool2d(2, 2)                          # 16x16x32
        
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # 16x16x64
        self.pool2 = nn.MaxPool2d(2, 2)                          # 8x8x64
        
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # 8x8x128
        self.pool3 = nn.MaxPool2d(2, 2)                           # 4x4x128
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * 4 * 4, 128)
        self.fc2 = nn.Linear(128, num_classes)
        
        # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ  
        self.dropout = nn.Dropout(0.5)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        x = F.relu(self.conv3(x))
        x = self.pool3(x)
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

def train_epoch(model, device, train_loader, optimizer, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)
        
        # æ‰“å°è¿›åº¦
        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data):5d}/{len(train_loader.dataset)} '
                  f'({100. * batch_idx / len(train_loader):3.0f}%)] '
                  f'Loss: {loss.item():.6f}')
    
    avg_loss = train_loss / len(train_loader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy

def test_epoch(model, device, test_loader):
    """æµ‹è¯•ä¸€ä¸ªepoch"""
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            output = model(data)
            test_loss += F.cross_entropy(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    
    test_loss /= total
    accuracy = 100. * correct / total
    
    return test_loss, accuracy

def plot_learning_curves(train_losses, train_accs, test_losses, test_accs, save_path='cnn_learning_curves.png'):
    """ç»˜åˆ¶å­¦ä¹ æ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    epochs = range(1, len(train_losses) + 1)
    
    # æŸå¤±æ›²çº¿
    ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
    ax1.plot(epochs, test_losses, 'r-', label='æµ‹è¯•æŸå¤±', linewidth=2)
    ax1.set_title('CNNæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, train_accs, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    ax2.plot(epochs, test_accs, 'r-', label='æµ‹è¯•å‡†ç¡®ç‡', linewidth=2)
    ax2.set_title('CNNå‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š å­¦ä¹ æ›²çº¿å·²ä¿å­˜è‡³: {save_path}")

def count_parameters(model):
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print("="*50)
    print("CNNæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # è¯¦ç»†åˆ†å±‚ç»Ÿè®¡
    print("\nå„å±‚å‚æ•°è¯¦æƒ…ï¼š")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"  {name}: {param.shape} -> {param.numel():,} å‚æ•°")
    
    print("="*50)
    
    return total_params, trainable_params

def visualize_feature_maps(model, device, sample_data, save_path='cnn_feature_maps.png'):
    """å¯è§†åŒ–å·ç§¯ç‰¹å¾å›¾"""
    model.eval()
    
    # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
    if len(sample_data.shape) == 3:
        sample_data = sample_data.unsqueeze(0)
    
    sample_data = sample_data.to(device)
    
    # å®šä¹‰hookå‡½æ•°æ¥è·å–ä¸­é—´å±‚è¾“å‡º
    activations = {}
    def get_activation(name):
        def hook(model, input, output):
            activations[name] = output.detach()
        return hook
    
    # æ³¨å†Œhook
    model.conv1.register_forward_hook(get_activation('conv1'))
    model.conv2.register_forward_hook(get_activation('conv2'))
    model.conv3.register_forward_hook(get_activation('conv3'))
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        _ = model(sample_data)
    
    # ç»˜åˆ¶ç‰¹å¾å›¾
    fig, axes = plt.subplots(3, 8, figsize=(16, 6))
    fig.suptitle('CNNå·ç§¯ç‰¹å¾å›¾å¯è§†åŒ–', fontsize=16, fontweight='bold')
    
    layers = ['conv1', 'conv2', 'conv3']
    for i, layer_name in enumerate(layers):
        feature_maps = activations[layer_name][0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        for j in range(8):  # æ˜¾ç¤ºå‰8ä¸ªé€šé“
            if j < feature_maps.shape[0]:
                feature_map = feature_maps[j].cpu().numpy()
                axes[i, j].imshow(feature_map, cmap='viridis')
                axes[i, j].set_title(f'{layer_name}[{j}]', fontsize=8)
            axes[i, j].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“Š ç‰¹å¾å›¾å¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}")

def main():
    print("ğŸš€ å¼€å§‹è®­ç»ƒå°è§„æ¨¡CNNåœ¨CIFAR-10æ•°æ®é›†ä¸Š")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è®¾ç½®è®¾å¤‡
    device = set_device()
    
    # è¶…å‚æ•°
    batch_size = 128
    test_batch_size = 256
    epochs = 25  # CNNé€šå¸¸éœ€è¦æ›´å¤šè½®æ¬¡
    learning_rate = 0.001
    
    print(f"ğŸ“‹ è¶…å‚æ•°è®¾ç½®:")
    print(f"  æ‰¹é‡å¤§å°: {batch_size}")
    print(f"  æµ‹è¯•æ‰¹é‡: {test_batch_size}")
    print(f"  è®­ç»ƒè½®æ¬¡: {epochs}")
    print(f"  å­¦ä¹ ç‡: {learning_rate}")
    print("="*60)
    
    # æ•°æ®é¢„å¤„ç† - CNNç‰ˆæœ¬ä½¿ç”¨æ›´å¼ºçš„æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomCrop(32, padding=4),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # å½’ä¸€åŒ–åˆ°[-1, 1]
    ])
    
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    # ä¸‹è½½å¹¶åŠ è½½CIFAR-10æ•°æ®é›†
    print("ğŸ“¥ ä¸‹è½½CIFAR-10æ•°æ®é›†...")
    train_dataset = datasets.CIFAR10(root='./data', train=True,
                                   download=True, transform=train_transform)
    test_dataset = datasets.CIFAR10(root='./data', train=False,
                                  download=True, transform=test_transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                            shuffle=True, num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=test_batch_size,
                           shuffle=False, num_workers=2, pin_memory=True)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_dataset):,}")
    print(f"  æµ‹è¯•æ ·æœ¬: {len(test_dataset):,}")
    print(f"  ç±»åˆ«æ•°é‡: 10")
    print("="*60)
    
    # CIFAR-10ç±»åˆ«åç§°
    classes = ('é£æœº', 'æ±½è½¦', 'é¸Ÿ', 'çŒ«', 'é¹¿', 'ç‹—', 'é’è›™', 'é©¬', 'èˆ¹', 'å¡è½¦')
    print(f"ğŸ·ï¸  ç±»åˆ«: {', '.join(classes)}")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    model = SimpleCNN(num_classes=10).to(device)
    
    # ç»Ÿè®¡å‚æ•°
    count_parameters(model)
    
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)
    
    # è®­ç»ƒè®°å½•
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []
    
    best_test_acc = 0.
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    total_start_time = time.time()
    
    for epoch in range(1, epochs + 1):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, epoch)
        
        # æµ‹è¯•
        test_loss, test_acc = test_epoch(model, device, test_loader)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), 'best_cnn_cifar10.pt')
        
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°epochç»“æœ
        print(f'Epoch {epoch:2d}/{epochs} | '
              f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:6.2f}% | '
              f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:6.2f}% | '
              f'Best: {best_test_acc:6.2f}% | LR: {scheduler.get_last_lr()[0]:.6f} | Time: {epoch_time:.1f}s')
        print("-" * 80)
    
    total_time = time.time() - total_start_time
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print("="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"ğŸ“ˆ æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: best_cnn_cifar10.pt")
    print("="*60)
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    plot_learning_curves(train_losses, train_accs, test_losses, test_accs)
    
    # ç‰¹å¾å›¾å¯è§†åŒ–
    sample_data, _ = next(iter(test_loader))
    visualize_feature_maps(model, device, sample_data[0])
    
    # æœ€ç»ˆç»“æœæ‘˜è¦
    print("\nğŸ“Š æœ€ç»ˆç»“æœæ‘˜è¦:")
    print(f"  æ¨¡å‹æ¶æ„: å°è§„æ¨¡CNN (3â†’32â†’64â†’128â†’FC)")
    print(f"  æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accs[-1]:.2f}%")
    print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accs[-1]:.2f}%")
    print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.2f}%")
    print(f"  è®­ç»ƒè½®æ¬¡: {epochs}")
    print(f"  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ä¸MLPå¯¹æ¯”åˆ†æ
    print("\nğŸ” ä¸MLPå¯¹æ¯”åˆ†æ:")
    print("  ä¼˜åŠ¿:")
    print("    â€¢ åˆ©ç”¨äº†ç©ºé—´å±€éƒ¨æ€§ - å·ç§¯æ“ä½œä¿ç•™äº†å›¾åƒçš„ç©ºé—´ç»“æ„")
    print("    â€¢ å¹³ç§»ä¸å˜æ€§ - å¯¹å›¾åƒä¸­å¯¹è±¡çš„ä½ç½®å˜åŒ–æ›´é²æ£’")
    print("    â€¢ å‚æ•°å…±äº« - ç›¸åŒçš„å·ç§¯æ ¸åœ¨æ•´ä¸ªå›¾åƒä¸Šå…±äº«æƒé‡")
    print("    â€¢ åˆ†å±‚ç‰¹å¾æå– - ä»ä½çº§è¾¹ç¼˜åˆ°é«˜çº§è¯­ä¹‰ç‰¹å¾")
    print("  é¢„æœŸè¡¨ç°:")
    print("    â€¢ æ”¶æ•›é€Ÿåº¦å¯èƒ½æ›´å¿«")
    print("    â€¢ æœ€ç»ˆå‡†ç¡®ç‡åº”è¯¥æ˜¾è‘—é«˜äºMLPçš„52.01%")
    print("    â€¢ å¯¹å›¾åƒæ•°æ®æ›´é€‚é…çš„å½’çº³åç½®")

if __name__ == "__main__":
    main()
