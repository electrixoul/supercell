'''
CIFAR-100 HyperNetwork ALL LAYERS VERSION
Based on cifar100_hypernetwork_fixed.py but with ALL conv and fc layers generated by hypernetworks
'''

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import time

def set_device():
    """Set optimal device"""
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS (Metal Performance Shaders) acceleration")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print("Using CUDA acceleration")
    else:
        device = torch.device("cpu")
        print("Using CPU")
    return device

class HyperCNNCIFAR_AllLayers(nn.Module):
    def __init__(self, f_size=5, in_size=32, out_size=64, z_dim=8):
        super(HyperCNNCIFAR_AllLayers, self).__init__()
        self.f_size = f_size
        self.in_size = in_size
        self.out_size = out_size
        self.z_dim = z_dim
        
        # Pooling
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)
        
        # Z signals for ALL layers - small initialization
        self.z_signal_conv1 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_conv2 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_conv3 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_fc1 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        self.z_signal_fc2 = nn.Parameter(torch.randn(1, z_dim) * 0.01)
        
        # Hypernetworks for generating weights
        # Conv1: 3 -> 32, kernel 5x5
        conv1_weight_size = in_size * 3 * f_size * f_size  # 32 * 3 * 5 * 5
        self.hyper_conv1 = nn.Linear(z_dim, conv1_weight_size)
        self.conv1_bias = nn.Parameter(torch.zeros(in_size))
        
        # Conv2: 32 -> 64, kernel 5x5
        conv2_weight_size = out_size * in_size * f_size * f_size  # 64 * 32 * 5 * 5
        self.hyper_conv2 = nn.Linear(z_dim, conv2_weight_size)
        self.conv2_bias = nn.Parameter(torch.zeros(out_size))
        
        # Conv3: 64 -> 128, kernel 3x3
        conv3_weight_size = 128 * out_size * 3 * 3  # 128 * 64 * 3 * 3
        self.hyper_conv3 = nn.Linear(z_dim, conv3_weight_size)
        self.conv3_bias = nn.Parameter(torch.zeros(128))
        
        # FC1: (128 * 4 * 4) -> 256
        fc1_input_size = 128 * 4 * 4  # 2048
        fc1_weight_size = 256 * fc1_input_size  # 256 * 2048
        self.hyper_fc1 = nn.Linear(z_dim, fc1_weight_size)
        self.fc1_bias = nn.Parameter(torch.zeros(256))
        
        # FC2: 256 -> 100
        fc2_weight_size = 100 * 256  # 100 * 256
        self.hyper_fc2 = nn.Linear(z_dim, fc2_weight_size)
        self.fc2_bias = nn.Parameter(torch.zeros(100))
        
        # Initialize hypernetworks with small weights
        self._init_hypernetworks()
        
        print(f"ALL LAYERS HyperNetwork Architecture:")
        print(f"  Conv1: z_dim={z_dim} -> {conv1_weight_size} params (32x3x5x5)")
        print(f"  Conv2: z_dim={z_dim} -> {conv2_weight_size} params (64x32x5x5)")
        print(f"  Conv3: z_dim={z_dim} -> {conv3_weight_size} params (128x64x3x3)")
        print(f"  FC1: z_dim={z_dim} -> {fc1_weight_size} params (256x2048)")
        print(f"  FC2: z_dim={z_dim} -> {fc2_weight_size} params (100x256)")
        
    def _init_hypernetworks(self):
        """Initialize all hypernetwork components with small weights"""
        hypernetworks = [
            self.hyper_conv1, self.hyper_conv2, self.hyper_conv3,
            self.hyper_fc1, self.hyper_fc2
        ]
        
        for hyper_net in hypernetworks:
            nn.init.normal_(hyper_net.weight, std=0.01)
            nn.init.constant_(hyper_net.bias, 0.0)
        
    def generate_conv1_weights(self):
        """Generate conv1 weights: 3 -> 32, kernel 5x5"""
        weights_flat = self.hyper_conv1(self.z_signal_conv1)
        return weights_flat.reshape(self.in_size, 3, self.f_size, self.f_size)
    
    def generate_conv2_weights(self):
        """Generate conv2 weights: 32 -> 64, kernel 5x5"""
        weights_flat = self.hyper_conv2(self.z_signal_conv2)
        return weights_flat.reshape(self.out_size, self.in_size, self.f_size, self.f_size)
    
    def generate_conv3_weights(self):
        """Generate conv3 weights: 64 -> 128, kernel 3x3"""
        weights_flat = self.hyper_conv3(self.z_signal_conv3)
        return weights_flat.reshape(128, self.out_size, 3, 3)
    
    def generate_fc1_weights(self):
        """Generate fc1 weights: 2048 -> 256"""
        weights_flat = self.hyper_fc1(self.z_signal_fc1)
        return weights_flat.reshape(256, 128 * 4 * 4)
    
    def generate_fc2_weights(self):
        """Generate fc2 weights: 256 -> 100"""
        weights_flat = self.hyper_fc2(self.z_signal_fc2)
        return weights_flat.reshape(100, 256)
        
    def forward(self, x):
        # Conv1 block (hypernetwork generated)
        conv1_weights = self.generate_conv1_weights()
        x = F.conv2d(x, conv1_weights, bias=self.conv1_bias, padding='same')
        x = self.pool(F.relu(x))      # 32->16
        
        # Conv2 block (hypernetwork generated)
        conv2_weights = self.generate_conv2_weights()
        x = F.conv2d(x, conv2_weights, bias=self.conv2_bias, padding='same')
        x = self.pool(F.relu(x))      # 16->8
        
        # Conv3 block (hypernetwork generated)
        conv3_weights = self.generate_conv3_weights()
        x = F.conv2d(x, conv3_weights, bias=self.conv3_bias, padding='same')
        x = self.pool(F.relu(x))      # 8->4
        
        # Flatten
        x = x.view(x.size(0), -1)
        
        # FC1 block (hypernetwork generated)
        fc1_weights = self.generate_fc1_weights()
        x = F.linear(x, fc1_weights, bias=self.fc1_bias)
        x = F.relu(x)
        x = self.dropout(x)
        
        # FC2 block (hypernetwork generated)
        fc2_weights = self.generate_fc2_weights()
        x = F.linear(x, fc2_weights, bias=self.fc2_bias)
        
        return x

def train_epoch(model, device, train_loader, optimizer, epoch, log_interval=50):
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)
        loss.backward()
        
        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        # Track metrics
        train_loss += loss.item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += target.size(0)
        
        if batch_idx % log_interval == 0:
            current_acc = 100. * correct / total
            print(f'  Batch: {batch_idx:3d}/{len(train_loader)} | '
                  f'Loss: {loss.item():.4f} | '
                  f'Acc: {current_acc:6.2f}%')
            
            # Debug hypernetwork signals
            conv_norms = [
                model.z_signal_conv1.norm().item(),
                model.z_signal_conv2.norm().item(),
                model.z_signal_conv3.norm().item()
            ]
            fc_norms = [
                model.z_signal_fc1.norm().item(),
                model.z_signal_fc2.norm().item()
            ]
            print(f'    Conv Z-norms: {conv_norms[0]:.4f}, {conv_norms[1]:.4f}, {conv_norms[2]:.4f}')
            print(f'    FC Z-norms: {fc_norms[0]:.4f}, {fc_norms[1]:.4f}')
    
    train_loss /= len(train_loader)
    train_acc = correct / total
    train_err = 1.0 - train_acc
    return train_loss, train_err

def evaluate(model, device, data_loader):
    model.eval()
    loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in data_loader:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            output = model(data)
            loss += F.cross_entropy(output, target, reduction='sum').item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
    
    loss /= total
    acc = correct / total
    err = 1.0 - acc
    return loss, err

def count_parameters(model):
    """Count and categorize parameters"""
    total_params = 0
    hyper_params = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            num_params = param.numel()
            total_params += num_params
            
            if 'z_signal' in name or 'hyper_' in name:
                hyper_params += num_params
                print(f"[HYPER] {name}: {param.shape}, {num_params:,}")
            else:
                print(f"[OTHER] {name}: {param.shape}, {num_params:,}")
    
    print(f"\nTotal trainable parameters: {total_params:,}")
    print(f"Hypernetwork parameters: {hyper_params:,}")
    print(f"Standard parameters: {total_params - hyper_params:,}")
    print(f"Hypernetwork ratio: {100.*hyper_params/total_params:.2f}%")
    return total_params, hyper_params

def main():
    print("Starting CIFAR-100 ALL LAYERS HyperNetwork...")
    print("ALL conv and fc layers generated by hypernetworks!")
    print("="*60)
    
    # Set seeds
    torch.manual_seed(42)
    np.random.seed(42)
    
    device = set_device()
    
    # Hyperparameters - keep similar to fixed version
    batch_size = 128
    test_batch_size = 256
    epochs = 50     # Start with fewer epochs to test
    lr = 0.001      # Same learning rate
    f_size = 5      
    in_size = 32    
    out_size = 64   
    z_dim = 8       # Same z_dim
    
    print(f"Hyperparameters:")
    print(f"  Batch size: {batch_size}")
    print(f"  Epochs: {epochs}")
    print(f"  Learning rate: {lr}")
    print(f"  Filter size: {f_size}")
    print(f"  Z dimension: {z_dim}")
    print("="*60)
    
    # Data transforms - same as fixed version
    transform_train = transforms.Compose([
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
    ])
    
    print("Loading CIFAR-100 dataset...")
    train_dataset = datasets.CIFAR100('data', train=True, download=False, transform=transform_train)
    test_dataset = datasets.CIFAR100('data', train=False, transform=transform_test)
    
    # Validation split
    train_size = len(train_dataset) - 5000
    train_dataset, val_dataset = torch.utils.data.random_split(
        train_dataset, [train_size, 5000]
    )
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=test_batch_size, num_workers=2, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, num_workers=2, pin_memory=True)
    
    print(f"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}")
    print("="*60)
    
    # Train ALL LAYERS HyperNetwork CNN
    print("Training ALL LAYERS HyperNetwork CNN...")
    model = HyperCNNCIFAR_AllLayers(f_size=f_size, in_size=in_size, out_size=out_size, z_dim=z_dim).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)
    
    print("\nALL LAYERS HyperNetwork CNN parameters:")
    count_parameters(model)
    print("="*60)
    
    best_val_err = 1.0
    for epoch in range(epochs):
        start_time = time.time()
        print(f"\nEpoch {epoch+1}/{epochs} - ALL LAYERS HyperNetwork CNN")
        print("-" * 50)
        
        train_loss, train_err = train_epoch(model, device, train_loader, optimizer, epoch)
        val_loss, val_err = evaluate(model, device, val_loader)
        scheduler.step()
        
        if val_err < best_val_err:
            best_val_err = val_err
            torch.save(model.state_dict(), 'cifar100_all_layers_hyper_best.pt')
            print(f"    *** New best validation error: {100*val_err:.2f}% - Model saved! ***")
        
        epoch_time = time.time() - start_time
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\nResults: Train Err: {100*train_err:.2f}% | Val Err: {100*val_err:.2f}% | "
              f"Best Val Err: {100*best_val_err:.2f}% | LR: {current_lr:.6f} | Time: {epoch_time:.1f}s")
    
    # Final evaluation
    print("\n" + "="*60)
    print("FINAL RESULTS")
    print("="*60)
    
    # Load best model and test
    model.load_state_dict(torch.load('cifar100_all_layers_hyper_best.pt'))
    test_loss, test_err = evaluate(model, device, test_loader)
    test_acc = (1 - test_err) * 100
    
    # Compare with previous results
    std_test_acc = 16.23   # Previous standard CNN result
    fixed_hyper_acc = 33.52  # Previous fixed hypernetwork result
    full_hyper_acc = 54.16   # Full hypernetwork result
    
    print(f"Standard CNN - Test Accuracy: {std_test_acc:.2f}% (previous result)")
    print(f"Fixed HyperNetwork (1 layer) - Test Accuracy: {fixed_hyper_acc:.2f}% (previous result)")
    print(f"Full HyperNetwork (all layers) - Test Accuracy: {full_hyper_acc:.2f}% (previous result)")
    print(f"ALL LAYERS HyperNetwork - Test Accuracy: {test_acc:.2f}%")
    
    improvement_vs_std = test_acc - std_test_acc
    improvement_vs_fixed = test_acc - fixed_hyper_acc
    improvement_vs_full = test_acc - full_hyper_acc
    
    print(f"Improvement vs Standard CNN: {improvement_vs_std:+.2f}%")
    print(f"Improvement vs Fixed HyperNet: {improvement_vs_fixed:+.2f}%")
    print(f"Improvement vs Full HyperNet: {improvement_vs_full:+.2f}%")
    
    # Print final z_signals
    print(f"\nFinal hypernetwork signals:")
    print(f"z_signal_conv1: {model.z_signal_conv1.detach().cpu().numpy().flatten()}")
    print(f"z_signal_conv2: {model.z_signal_conv2.detach().cpu().numpy().flatten()}")
    print(f"z_signal_conv3: {model.z_signal_conv3.detach().cpu().numpy().flatten()}")
    print(f"z_signal_fc1: {model.z_signal_fc1.detach().cpu().numpy().flatten()}")
    print(f"z_signal_fc2: {model.z_signal_fc2.detach().cpu().numpy().flatten()}")
    
    print("\nTraining completed!")

if __name__ == "__main__":
    main()
